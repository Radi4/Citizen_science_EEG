{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D, UpSampling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from mne.io import read_raw_brainvision as mne_read\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channels_num = 60\n",
    "window_size = 500\n",
    "nb_epoch = 50\n",
    "\n",
    "# Hyper parameters\n",
    "batch_size = 32\n",
    "nb_visible = channels_num * window_size\n",
    "nb_hidden = 1000\n",
    "nb_emb = 20\n",
    "\n",
    "corruption_level = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mne_format_data_dir = join('data', 'resting_state_mne_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inp = Input(shape=(window_size, channels_num, 1))\n",
    "\n",
    "    encoder = Sequential( [\n",
    "        Conv2D(128, kernel_size=(5,5), activation='relu', padding='same', input_shape=(window_size,channels_num,1)),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(1, kernel_size=(1,1), padding='same'),\n",
    "    ] )\n",
    "    print(encoder.summary())\n",
    "    decoder = Sequential((\n",
    "        Conv2D(128, kernel_size=(5,5), activation='relu', padding='same', input_shape=(125, 15, 1) ),\n",
    "        UpSampling2D(),\n",
    "        Conv2D(64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "        UpSampling2D(),\n",
    "        Conv2D(1, kernel_size=(1,1), padding='same'),\n",
    "    ))\n",
    "    print(decoder.summary())\n",
    "    autoencoder = Model(inp, decoder(encoder(inp)))\n",
    "    autoencoder.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    print(autoencoder.summary())\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 500, 60, 128)      3328      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 250, 30, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 250, 30, 64)       73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 125, 15, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 125, 15, 1)        65        \n",
      "=================================================================\n",
      "Total params: 77,185\n",
      "Trainable params: 77,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 125, 15, 128)      3328      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 250, 30, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 250, 30, 64)       73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 500, 60, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 500, 60, 1)        65        \n",
      "=================================================================\n",
      "Total params: 77,185\n",
      "Trainable params: 77,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500, 60, 1)        0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 125, 15, 1)        77185     \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 500, 60, 1)        77185     \n",
      "=================================================================\n",
      "Total params: 154,370\n",
      "Trainable params: 154,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "autoencoder = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "mean, var = None, None\n",
    "\n",
    "def cut_window(data):\n",
    "    return data[:, :data.shape[1] - data.shape[1] % window_size]\n",
    "\n",
    "def read_func(filename, verbose=0):\n",
    "    d = mne_read(filename, verbose=verbose, preload=True)\n",
    "    d.drop_channels(['VEOG', 'HEOG', 'STI 014'])\n",
    "    d = d.get_data()\n",
    "    return cut_window(np.vstack( [d, np.zeros( (2, d.shape[1]) )]) )\n",
    "\n",
    "def read_data(data_type, return_batch_np=False):\n",
    "    global mean, var\n",
    "    ''' If return_batch_np is False\n",
    "            return data with shape (None, batch_size, nb_visible)\n",
    "        else number of batches in data\n",
    "    '''\n",
    "    verbose = None if return_batch_np else 0\n",
    "    \n",
    "    data_dir = join(mne_format_data_dir, data_type)\n",
    "    data = np.hstack([read_func( join(data_dir, fn), verbose=verbose) for fn in listdir(data_dir) if fn[-5:] == '.vhdr'])\n",
    "    \n",
    "    if mean is None:\n",
    "        mean, var = data.mean(), data.var()\n",
    "    #data = scaler.transform(data)\n",
    "    data = (data - mean) / (var ** (1/2))\n",
    "    print(data_type, data.mean(), data.var())\n",
    "    if return_batch_np:\n",
    "        return data.shape[1] // window_size // batch_size\n",
    "    return data.T.reshape(-1, window_size, channels_num, 1)\n",
    "\n",
    "def gen(data_type):\n",
    "    data = read_data(data_type)\n",
    "    print(data.shape)\n",
    "    for ep in range(nb_epoch+1):\n",
    "        for i in range(data.shape[0] // batch_size):\n",
    "            x = data[i*batch_size:(i+1)*batch_size, :, :, :]\n",
    "            yield x, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train -6.34378883316e-16 1.0\n",
      "(29879, 500, 60, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[-0.34265339],\n",
       "          [ 0.07031351],\n",
       "          [-0.22499868],\n",
       "          ..., \n",
       "          [-0.36359731],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34293028],\n",
       "          [ 0.07011224],\n",
       "          [-0.22517771],\n",
       "          ..., \n",
       "          [-0.3636607 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34322052],\n",
       "          [ 0.06987315],\n",
       "          [-0.22535897],\n",
       "          ..., \n",
       "          [-0.36362845],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.34291805],\n",
       "          [ 0.0695173 ],\n",
       "          [-0.22572594],\n",
       "          ..., \n",
       "          [-0.36324369],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34231756],\n",
       "          [ 0.06972636],\n",
       "          [-0.22543904],\n",
       "          ..., \n",
       "          [-0.36279665],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34207736],\n",
       "          [ 0.06995099],\n",
       "          [-0.22520329],\n",
       "          ..., \n",
       "          [-0.36214945],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.34273345],\n",
       "          [ 0.07007109],\n",
       "          [-0.22498088],\n",
       "          ..., \n",
       "          [-0.36200489],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34317159],\n",
       "          [ 0.0701178 ],\n",
       "          [-0.2248252 ],\n",
       "          ..., \n",
       "          [-0.3623207 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34353523],\n",
       "          [ 0.06994321],\n",
       "          [-0.22498644],\n",
       "          ..., \n",
       "          [-0.3628289 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.34314824],\n",
       "          [ 0.07024346],\n",
       "          [-0.22537121],\n",
       "          ..., \n",
       "          [-0.36286448],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34238761],\n",
       "          [ 0.07050367],\n",
       "          [-0.22511544],\n",
       "          ..., \n",
       "          [-0.36266209],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34214408],\n",
       "          [ 0.07056706],\n",
       "          [-0.22494308],\n",
       "          ..., \n",
       "          [-0.36275439],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.34269564],\n",
       "          [ 0.07033576],\n",
       "          [-0.22505428],\n",
       "          ..., \n",
       "          [-0.36318141],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34298255],\n",
       "          [ 0.07015116],\n",
       "          [-0.2252411 ],\n",
       "          ..., \n",
       "          [-0.36364179],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34357081],\n",
       "          [ 0.0699054 ],\n",
       "          [-0.22541124],\n",
       "          ..., \n",
       "          [-0.36389645],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.34174153],\n",
       "          [ 0.07161903],\n",
       "          [-0.22378212],\n",
       "          ..., \n",
       "          [-0.36134879],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34114437],\n",
       "          [ 0.07195598],\n",
       "          [-0.22352302],\n",
       "          ..., \n",
       "          [-0.36104076],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34100092],\n",
       "          [ 0.071986  ],\n",
       "          [-0.22355749],\n",
       "          ..., \n",
       "          [-0.36101073],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[-0.32129023],\n",
       "          [ 0.09197692],\n",
       "          [-0.20300834],\n",
       "          ..., \n",
       "          [-0.33943852],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32152487],\n",
       "          [ 0.09183347],\n",
       "          [-0.20319405],\n",
       "          ..., \n",
       "          [-0.33977769],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32164052],\n",
       "          [ 0.09166889],\n",
       "          [-0.20331192],\n",
       "          ..., \n",
       "          [-0.33978881],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.3196689 ],\n",
       "          [ 0.0925396 ],\n",
       "          [-0.20261913],\n",
       "          ..., \n",
       "          [-0.33876018],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32016375],\n",
       "          [ 0.09232276],\n",
       "          [-0.20288045],\n",
       "          ..., \n",
       "          [-0.33887917],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32053517],\n",
       "          [ 0.09219376],\n",
       "          [-0.20304503],\n",
       "          ..., \n",
       "          [-0.33924169],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.32088657],\n",
       "          [ 0.09201584],\n",
       "          [-0.20300722],\n",
       "          ..., \n",
       "          [-0.33955528],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32120794],\n",
       "          [ 0.09191687],\n",
       "          [-0.20301501],\n",
       "          ..., \n",
       "          [-0.33962979],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32120238],\n",
       "          [ 0.09190686],\n",
       "          [-0.2029583 ],\n",
       "          ..., \n",
       "          [-0.33925948],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.31963665],\n",
       "          [ 0.09296106],\n",
       "          [-0.20220879],\n",
       "          ..., \n",
       "          [-0.3386145 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.31984904],\n",
       "          [ 0.09287766],\n",
       "          [-0.20230331],\n",
       "          ..., \n",
       "          [-0.33864898],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32005032],\n",
       "          [ 0.09278314],\n",
       "          [-0.202469  ],\n",
       "          ..., \n",
       "          [-0.33872348],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.32051181],\n",
       "          [ 0.09250847],\n",
       "          [-0.20265916],\n",
       "          ..., \n",
       "          [-0.33889362],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32074423],\n",
       "          [ 0.09244508],\n",
       "          [-0.20259355],\n",
       "          ..., \n",
       "          [-0.33885693],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32072532],\n",
       "          [ 0.0922616 ],\n",
       "          [-0.20267473],\n",
       "          ..., \n",
       "          [-0.33889362],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.31875259],\n",
       "          [ 0.09353598],\n",
       "          [-0.20171394],\n",
       "          ..., \n",
       "          [-0.33781941],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.3190595 ],\n",
       "          [ 0.09328021],\n",
       "          [-0.20180512],\n",
       "          ..., \n",
       "          [-0.337597  ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.31926857],\n",
       "          [ 0.09323796],\n",
       "          [-0.20169837],\n",
       "          ..., \n",
       "          [-0.33746356],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]]]), array([[[[-0.34265339],\n",
       "          [ 0.07031351],\n",
       "          [-0.22499868],\n",
       "          ..., \n",
       "          [-0.36359731],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34293028],\n",
       "          [ 0.07011224],\n",
       "          [-0.22517771],\n",
       "          ..., \n",
       "          [-0.3636607 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34322052],\n",
       "          [ 0.06987315],\n",
       "          [-0.22535897],\n",
       "          ..., \n",
       "          [-0.36362845],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.34291805],\n",
       "          [ 0.0695173 ],\n",
       "          [-0.22572594],\n",
       "          ..., \n",
       "          [-0.36324369],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34231756],\n",
       "          [ 0.06972636],\n",
       "          [-0.22543904],\n",
       "          ..., \n",
       "          [-0.36279665],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34207736],\n",
       "          [ 0.06995099],\n",
       "          [-0.22520329],\n",
       "          ..., \n",
       "          [-0.36214945],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.34273345],\n",
       "          [ 0.07007109],\n",
       "          [-0.22498088],\n",
       "          ..., \n",
       "          [-0.36200489],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34317159],\n",
       "          [ 0.0701178 ],\n",
       "          [-0.2248252 ],\n",
       "          ..., \n",
       "          [-0.3623207 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34353523],\n",
       "          [ 0.06994321],\n",
       "          [-0.22498644],\n",
       "          ..., \n",
       "          [-0.3628289 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.34314824],\n",
       "          [ 0.07024346],\n",
       "          [-0.22537121],\n",
       "          ..., \n",
       "          [-0.36286448],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34238761],\n",
       "          [ 0.07050367],\n",
       "          [-0.22511544],\n",
       "          ..., \n",
       "          [-0.36266209],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34214408],\n",
       "          [ 0.07056706],\n",
       "          [-0.22494308],\n",
       "          ..., \n",
       "          [-0.36275439],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.34269564],\n",
       "          [ 0.07033576],\n",
       "          [-0.22505428],\n",
       "          ..., \n",
       "          [-0.36318141],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34298255],\n",
       "          [ 0.07015116],\n",
       "          [-0.2252411 ],\n",
       "          ..., \n",
       "          [-0.36364179],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34357081],\n",
       "          [ 0.0699054 ],\n",
       "          [-0.22541124],\n",
       "          ..., \n",
       "          [-0.36389645],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.34174153],\n",
       "          [ 0.07161903],\n",
       "          [-0.22378212],\n",
       "          ..., \n",
       "          [-0.36134879],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34114437],\n",
       "          [ 0.07195598],\n",
       "          [-0.22352302],\n",
       "          ..., \n",
       "          [-0.36104076],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.34100092],\n",
       "          [ 0.071986  ],\n",
       "          [-0.22355749],\n",
       "          ..., \n",
       "          [-0.36101073],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        ..., \n",
       "        [[[-0.32129023],\n",
       "          [ 0.09197692],\n",
       "          [-0.20300834],\n",
       "          ..., \n",
       "          [-0.33943852],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32152487],\n",
       "          [ 0.09183347],\n",
       "          [-0.20319405],\n",
       "          ..., \n",
       "          [-0.33977769],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32164052],\n",
       "          [ 0.09166889],\n",
       "          [-0.20331192],\n",
       "          ..., \n",
       "          [-0.33978881],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.3196689 ],\n",
       "          [ 0.0925396 ],\n",
       "          [-0.20261913],\n",
       "          ..., \n",
       "          [-0.33876018],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32016375],\n",
       "          [ 0.09232276],\n",
       "          [-0.20288045],\n",
       "          ..., \n",
       "          [-0.33887917],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32053517],\n",
       "          [ 0.09219376],\n",
       "          [-0.20304503],\n",
       "          ..., \n",
       "          [-0.33924169],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.32088657],\n",
       "          [ 0.09201584],\n",
       "          [-0.20300722],\n",
       "          ..., \n",
       "          [-0.33955528],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32120794],\n",
       "          [ 0.09191687],\n",
       "          [-0.20301501],\n",
       "          ..., \n",
       "          [-0.33962979],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32120238],\n",
       "          [ 0.09190686],\n",
       "          [-0.2029583 ],\n",
       "          ..., \n",
       "          [-0.33925948],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.31963665],\n",
       "          [ 0.09296106],\n",
       "          [-0.20220879],\n",
       "          ..., \n",
       "          [-0.3386145 ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.31984904],\n",
       "          [ 0.09287766],\n",
       "          [-0.20230331],\n",
       "          ..., \n",
       "          [-0.33864898],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32005032],\n",
       "          [ 0.09278314],\n",
       "          [-0.202469  ],\n",
       "          ..., \n",
       "          [-0.33872348],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]],\n",
       " \n",
       " \n",
       "        [[[-0.32051181],\n",
       "          [ 0.09250847],\n",
       "          [-0.20265916],\n",
       "          ..., \n",
       "          [-0.33889362],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32074423],\n",
       "          [ 0.09244508],\n",
       "          [-0.20259355],\n",
       "          ..., \n",
       "          [-0.33885693],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.32072532],\n",
       "          [ 0.0922616 ],\n",
       "          [-0.20267473],\n",
       "          ..., \n",
       "          [-0.33889362],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         ..., \n",
       "         [[-0.31875259],\n",
       "          [ 0.09353598],\n",
       "          [-0.20171394],\n",
       "          ..., \n",
       "          [-0.33781941],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.3190595 ],\n",
       "          [ 0.09328021],\n",
       "          [-0.20180512],\n",
       "          ..., \n",
       "          [-0.337597  ],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]],\n",
       " \n",
       "         [[-0.31926857],\n",
       "          [ 0.09323796],\n",
       "          [-0.20169837],\n",
       "          ..., \n",
       "          [-0.33746356],\n",
       "          [-0.11140064],\n",
       "          [-0.11140064]]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from data/resting_state_mne_format/train/2505_shirokova_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 606599  =      0.000 ...   606.599 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/miloslavov_22_05_pre_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603399  =      0.000 ...   603.399 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/gorin_rest_eeg_post_31011200.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603699  =      0.000 ...   603.699 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2704_zagirova_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 604549  =      0.000 ...   604.549 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2103_glebko_posteeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 633899  =      0.000 ...   633.899 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/200317_ivanova_pre_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 602099  =      0.000 ...   602.099 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/gorbacheva_03_02_2017_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603949  =      0.000 ...   603.949 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2403_kutuzova_pre_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 602849  =      0.000 ...   602.849 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/08021400_post_eeg_shuhova.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 607199  =      0.000 ...   607.199 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2103_kozunova_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603199  =      0.000 ...   603.199 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/shuhova_08022017_rest_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 637949  =      0.000 ...   637.949 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/gorbacheva_03021300_rest_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 625249  =      0.000 ...   625.249 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/pre_egg_petuhova3103.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 602749  =      0.000 ...   602.749 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/zavrin_open_eyes_eeg_15021500.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 301999  =      0.000 ...   301.999 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2003_ivanova_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 616599  =      0.000 ...   616.599 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2403_kutuzova_posteeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 604199  =      0.000 ...   604.199 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/gorin_310117_rest_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 609999  =      0.000 ...   609.999 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/post_eeg_tsoy_2504.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 604949  =      0.000 ...   604.949 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/dagaev_post_rest_eeg_30011600.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603799  =      0.000 ...   603.799 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/glebko_2103_pre_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 643749  =      0.000 ...   643.749 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/2505_shirokova.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 604799  =      0.000 ...   604.799 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/kozunova_pre_eeg_2103.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 602849  =      0.000 ...   602.849 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/bagaeva_post_eeg_13031500.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603899  =      0.000 ...   603.899 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/bagaeva_13021500_rest_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 603949  =      0.000 ...   603.949 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/train/zagirova_pre_eeg_2704.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 607549  =      0.000 ...   607.549 secs...\n",
      "train -6.34378883316e-16 1.0\n",
      "Extracting parameters from data/resting_state_mne_format/validation/3103_petuhova_posteeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 630049  =      0.000 ...   630.049 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/validation/zavrib_post_eeg_eyesopen15021500.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 321599  =      0.000 ...   321.599 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/validation/2205_miloslavov_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 623249  =      0.000 ...   623.249 secs...\n",
      "validation -0.164368573828 0.135983946103\n"
     ]
    }
   ],
   "source": [
    "train_batches_nb = read_data('train', return_batch_np=True)\n",
    "val_batches_nb = read_data('validation', return_batch_np=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "train -6.34378883316e-16 1.0\n",
      "(29879, 500, 60, 1)\n",
      "932/933 [============================>.] - ETA: 0s - loss: 0.6052 - mean_absolute_error: 0.2604validation -0.164368573828 0.135983946103\n",
      "(3149, 500, 60, 1)\n",
      "933/933 [==============================] - 394s - loss: 0.6046 - mean_absolute_error: 0.2603 - val_loss: 0.1057 - val_mean_absolute_error: 0.2577\n",
      "Epoch 2/50\n",
      "933/933 [==============================] - 349s - loss: 0.5680 - mean_absolute_error: 0.2470 - val_loss: 0.0953 - val_mean_absolute_error: 0.2452\n",
      "Epoch 3/50\n",
      "933/933 [==============================] - 349s - loss: 0.5183 - mean_absolute_error: 0.2336 - val_loss: 0.0901 - val_mean_absolute_error: 0.2411\n",
      "Epoch 4/50\n",
      "933/933 [==============================] - 350s - loss: 0.5124 - mean_absolute_error: 0.2287 - val_loss: 0.0903 - val_mean_absolute_error: 0.2402\n",
      "Epoch 5/50\n",
      "933/933 [==============================] - 349s - loss: 0.5103 - mean_absolute_error: 0.2263 - val_loss: 0.0916 - val_mean_absolute_error: 0.2418\n",
      "Epoch 6/50\n",
      "933/933 [==============================] - 349s - loss: 0.5110 - mean_absolute_error: 0.2259 - val_loss: 0.0875 - val_mean_absolute_error: 0.2390\n",
      "Epoch 7/50\n",
      "933/933 [==============================] - 348s - loss: 0.5122 - mean_absolute_error: 0.2265 - val_loss: 0.0919 - val_mean_absolute_error: 0.2398\n",
      "Epoch 8/50\n",
      "933/933 [==============================] - 348s - loss: 0.5095 - mean_absolute_error: 0.2253 - val_loss: 0.0886 - val_mean_absolute_error: 0.2380\n",
      "Epoch 9/50\n",
      "933/933 [==============================] - 349s - loss: 0.5090 - mean_absolute_error: 0.2240 - val_loss: 0.0953 - val_mean_absolute_error: 0.2481\n",
      "Epoch 10/50\n",
      "933/933 [==============================] - 348s - loss: 0.5090 - mean_absolute_error: 0.2228 - val_loss: 0.0936 - val_mean_absolute_error: 0.2462\n",
      "Epoch 11/50\n",
      "933/933 [==============================] - 348s - loss: 0.5091 - mean_absolute_error: 0.2230 - val_loss: 0.0930 - val_mean_absolute_error: 0.2504\n",
      "Epoch 12/50\n",
      "933/933 [==============================] - 348s - loss: 0.5082 - mean_absolute_error: 0.2225 - val_loss: 0.1102 - val_mean_absolute_error: 0.2640\n",
      "Epoch 13/50\n",
      "933/933 [==============================] - 347s - loss: 0.5094 - mean_absolute_error: 0.2220 - val_loss: 0.0980 - val_mean_absolute_error: 0.2495\n",
      "Epoch 14/50\n",
      "933/933 [==============================] - 347s - loss: 0.5075 - mean_absolute_error: 0.2201 - val_loss: 0.0864 - val_mean_absolute_error: 0.2385\n",
      "Epoch 15/50\n",
      "933/933 [==============================] - 347s - loss: 0.5066 - mean_absolute_error: 0.2188 - val_loss: 0.0930 - val_mean_absolute_error: 0.2445\n",
      "Epoch 16/50\n",
      "933/933 [==============================] - 346s - loss: 0.5075 - mean_absolute_error: 0.2191 - val_loss: 0.0918 - val_mean_absolute_error: 0.2501\n",
      "Epoch 17/50\n",
      "933/933 [==============================] - 346s - loss: 0.5071 - mean_absolute_error: 0.2195 - val_loss: 0.0908 - val_mean_absolute_error: 0.2402\n",
      "Epoch 18/50\n",
      "933/933 [==============================] - 346s - loss: 0.5061 - mean_absolute_error: 0.2179 - val_loss: 0.0953 - val_mean_absolute_error: 0.2470\n",
      "Epoch 19/50\n",
      "933/933 [==============================] - 347s - loss: 0.5056 - mean_absolute_error: 0.2180 - val_loss: 0.0921 - val_mean_absolute_error: 0.2435\n",
      "Epoch 20/50\n",
      "933/933 [==============================] - 346s - loss: 0.5058 - mean_absolute_error: 0.2172 - val_loss: 0.0853 - val_mean_absolute_error: 0.2330\n",
      "Epoch 21/50\n",
      "933/933 [==============================] - 345s - loss: 0.5052 - mean_absolute_error: 0.2161 - val_loss: 0.0859 - val_mean_absolute_error: 0.2361\n",
      "Epoch 22/50\n",
      "933/933 [==============================] - 346s - loss: 0.5066 - mean_absolute_error: 0.2160 - val_loss: 0.0914 - val_mean_absolute_error: 0.2421\n",
      "Epoch 23/50\n",
      "933/933 [==============================] - 345s - loss: 0.5044 - mean_absolute_error: 0.2150 - val_loss: 0.0919 - val_mean_absolute_error: 0.2436\n",
      "Epoch 24/50\n",
      "933/933 [==============================] - 345s - loss: 0.5047 - mean_absolute_error: 0.2141 - val_loss: 0.0942 - val_mean_absolute_error: 0.2417\n",
      "Epoch 25/50\n",
      "933/933 [==============================] - 345s - loss: 0.5048 - mean_absolute_error: 0.2150 - val_loss: 0.0844 - val_mean_absolute_error: 0.2386\n",
      "Epoch 26/50\n",
      "933/933 [==============================] - 345s - loss: 0.5056 - mean_absolute_error: 0.2150 - val_loss: 0.0926 - val_mean_absolute_error: 0.2425\n",
      "Epoch 27/50\n",
      "933/933 [==============================] - 346s - loss: 0.5045 - mean_absolute_error: 0.2143 - val_loss: 0.0955 - val_mean_absolute_error: 0.2461\n",
      "Epoch 28/50\n",
      "933/933 [==============================] - 345s - loss: 0.5042 - mean_absolute_error: 0.2140 - val_loss: 0.0854 - val_mean_absolute_error: 0.2395\n",
      "Epoch 29/50\n",
      "933/933 [==============================] - 344s - loss: 0.5036 - mean_absolute_error: 0.2136 - val_loss: 0.0864 - val_mean_absolute_error: 0.2388\n",
      "Epoch 30/50\n",
      "933/933 [==============================] - 344s - loss: 0.5044 - mean_absolute_error: 0.2133 - val_loss: 0.0899 - val_mean_absolute_error: 0.2454\n",
      "Epoch 31/50\n",
      "933/933 [==============================] - 345s - loss: 0.5036 - mean_absolute_error: 0.2128 - val_loss: 0.0904 - val_mean_absolute_error: 0.2442\n",
      "Epoch 32/50\n",
      "933/933 [==============================] - 344s - loss: 0.5031 - mean_absolute_error: 0.2127 - val_loss: 0.0955 - val_mean_absolute_error: 0.2528\n",
      "Epoch 33/50\n",
      "933/933 [==============================] - 344s - loss: 0.5027 - mean_absolute_error: 0.2119 - val_loss: 0.0981 - val_mean_absolute_error: 0.2532\n",
      "Epoch 34/50\n",
      "933/933 [==============================] - 345s - loss: 0.5029 - mean_absolute_error: 0.2120 - val_loss: 0.0965 - val_mean_absolute_error: 0.2498\n",
      "Epoch 35/50\n",
      "933/933 [==============================] - 345s - loss: 0.5025 - mean_absolute_error: 0.2115 - val_loss: 0.0889 - val_mean_absolute_error: 0.2409\n",
      "Epoch 36/50\n",
      "933/933 [==============================] - 344s - loss: 0.5024 - mean_absolute_error: 0.2112 - val_loss: 0.0906 - val_mean_absolute_error: 0.2420\n",
      "Epoch 37/50\n",
      "933/933 [==============================] - 345s - loss: 0.5020 - mean_absolute_error: 0.2106 - val_loss: 0.0884 - val_mean_absolute_error: 0.2373\n",
      "Epoch 38/50\n",
      "933/933 [==============================] - 345s - loss: 0.5042 - mean_absolute_error: 0.2124 - val_loss: 0.0915 - val_mean_absolute_error: 0.2423\n",
      "Epoch 39/50\n",
      "933/933 [==============================] - 344s - loss: 0.5038 - mean_absolute_error: 0.2119 - val_loss: 0.1003 - val_mean_absolute_error: 0.2441\n",
      "Epoch 40/50\n",
      "933/933 [==============================] - 345s - loss: 0.5026 - mean_absolute_error: 0.2111 - val_loss: 0.0935 - val_mean_absolute_error: 0.2393\n",
      "Epoch 41/50\n",
      "933/933 [==============================] - 344s - loss: 0.5024 - mean_absolute_error: 0.2111 - val_loss: 0.0911 - val_mean_absolute_error: 0.2358\n",
      "Epoch 42/50\n",
      "933/933 [==============================] - 344s - loss: 0.5022 - mean_absolute_error: 0.2106 - val_loss: 0.0883 - val_mean_absolute_error: 0.2342\n",
      "Epoch 43/50\n",
      "933/933 [==============================] - 344s - loss: 0.5024 - mean_absolute_error: 0.2105 - val_loss: 0.0908 - val_mean_absolute_error: 0.2379\n",
      "Epoch 44/50\n",
      "933/933 [==============================] - 344s - loss: 0.5019 - mean_absolute_error: 0.2099 - val_loss: 0.0922 - val_mean_absolute_error: 0.2364\n",
      "Epoch 45/50\n",
      "933/933 [==============================] - 344s - loss: 0.5022 - mean_absolute_error: 0.2110 - val_loss: 0.0909 - val_mean_absolute_error: 0.2362\n",
      "Epoch 46/50\n",
      "933/933 [==============================] - 344s - loss: 0.5018 - mean_absolute_error: 0.2099 - val_loss: 0.0945 - val_mean_absolute_error: 0.2391\n",
      "Epoch 47/50\n",
      "932/933 [============================>.] - ETA: 0s - loss: 0.5038 - mean_absolute_error: 0.2102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-51:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 560, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "StopIteration\n",
      "\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-980e1844caf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = autoencoder.fit_generator(gen('train'), steps_per_epoch=train_batches_nb, epochs=nb_epoch,                           validation_data=gen('validation'), validation_steps=val_batches_nb,                           #use_multiprocessing=True, \\\n\u001b[0;32m----> 2\u001b[0;31m                           callbacks=[ModelCheckpoint(\"models/cnn_%s_{epoch:02d}.hdf5\"%str(nb_hidden), save_best_only=False)])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1860\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1863\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                     raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit_generator(gen('train'), steps_per_epoch=train_batches_nb, epochs=nb_epoch, \\\n",
    "                          validation_data=gen('validation'), validation_steps=val_batches_nb, \\\n",
    "                          #use_multiprocessing=True, \\\n",
    "                          callbacks=[ModelCheckpoint(\"models/cnn_%s_{epoch:02d}.hdf5\"%str(nb_hidden), save_best_only=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = next(gen('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder.predict(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from data/resting_state_mne_format/test/zavrin_eyes_closed_eeg_15021500.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 302749  =      0.000 ...   302.749 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/test/zavrin_15021500_eyesclosed_post_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 319299  =      0.000 ...   319.299 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/test/300120171600_dagaev_rest_eeg.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 615599  =      0.000 ...   615.599 secs...\n",
      "Extracting parameters from data/resting_state_mne_format/test/tsoy_pre_eeg_2504.vhdr...\n",
      "Setting channel info structure...\n",
      "Reading 0 ... 652699  =      0.000 ...   652.699 secs...\n",
      "test 5.32180965766e-17 1.0\n"
     ]
    }
   ],
   "source": [
    "test_batches_nb = read_data('test', return_batch_np=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "autoencoder = load_model('models/cnn_1000_06.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 5.32180965766e-17 1.0\n",
      "(3779, 500, 60, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.60504258853399151, 0.58064251835063352]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.evaluate_generator(gen('test'), steps=test_batches_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'mean_absolute_error']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = next(gen('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 61)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhKMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f780359a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_test(n):\n",
    "    plt.plot(np.arange(test[0][n].size), (test[0][n] * var ** (1/2) + mean).flatten(), color='b')\n",
    "    res = autoencoder.predict(test[0][n].reshape(1, -1))\n",
    "    plt.plot(np.arange(res.size), (res * var ** (1/2) + mean).flatten(), color='r')\n",
    "    plt.show()\n",
    "    \n",
    "show_test(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19735468, -1.19766106, -1.1981571 , -1.19840395, -1.19839296,\n",
       "       -1.19832731, -1.19817781, -1.19847933, -1.19851823, -1.19842214,\n",
       "       -1.19833343, -1.1980574 , -1.19771942, -1.19723923, -1.19724166,\n",
       "       -1.1972793 , -1.1971869 , -1.19685378, -1.19702642, -1.19726958,\n",
       "       -1.19745194, -1.19777417, -1.19786414, -1.19775832, -1.19789935,\n",
       "       -1.19814863, -1.19816809, -1.19852309, -1.1986118 , -1.19873707,\n",
       "       -1.19857532, -1.19835892, -1.19795411, -1.19737783, -1.19721365,\n",
       "       -1.19723554, -1.19742403, -1.19722338, -1.19716016, -1.19750058,\n",
       "       -1.19784099, -1.19789332, -1.19791394, -1.1980076 , -1.19825076,\n",
       "       -1.19836135, -1.19826535, -1.19845861, -1.19871762, -1.19881605,\n",
       "       -1.19838566, -1.1978264 , -1.19747996, -1.1974143 , -1.19754075,\n",
       "       -1.1975991 , -1.19777777, -1.1974143 , -1.19744465, -1.1975565 ,\n",
       "       -1.19786171])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.81068474, -0.80941945, -0.81093025, -0.81085229, -0.80660617,\n",
       "        -0.81327504, -0.81492221, -0.80790353, -0.80843753, -0.80828446,\n",
       "        -0.79405653, -0.79962194, -0.81512022, -0.80202258, -0.80774528,\n",
       "        -0.80601555, -0.80853337, -0.80269289, -0.81089044, -0.81020975,\n",
       "        -0.80502218, -0.80605251, -0.81084102, -0.8100906 , -0.80870497,\n",
       "        -0.79747999, -0.80827975, -0.79939252, -0.80619758, -0.80837649,\n",
       "        -0.80311096, -0.80861604, -0.8124631 , -0.8023845 , -0.80139577,\n",
       "        -0.80273634, -0.79608792, -0.7938416 , -0.81079549, -0.79689747,\n",
       "        -0.80076027, -0.8090685 , -0.80986834, -0.79452288, -0.80349278,\n",
       "        -0.80141538, -0.81425375, -0.79669464, -0.80475599, -0.80646384,\n",
       "        -0.7984153 , -0.79969573, -0.80898458, -0.7993139 , -0.8075695 ,\n",
       "        -0.80733687, -0.80190003, -0.80580699, -0.80462283, -0.80776191,\n",
       "        -0.81464827]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.predict(test[0][0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027330162475785683"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.evaluate_generator(gen('test'), steps=test_batches_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.vstack([x[:, :x.shape[1]-(x.shape[1] % window_size)].T.reshape(-1, window_size, channels_num) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEBUG: this code is for debugging here, delete it or ignore\n",
    "# scaler = StandardScaler()\n",
    "# for el in data:  #DEBUG: iterate over all list\n",
    "#     scaler.partial_fit(el.reshape(-1, 1))\n",
    "# fir = scaler.transform(data[0].reshape(-1, 1))\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#data_scaled = scaler.fit_transform(data.reshape(-1, 1)).reshape(-1, nb_visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for el in gen(data_type='train'):\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    for i in range(x):\n",
    "        yield i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for e in f(7):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98845 samples, validate on 10983 samples\n",
      "Epoch 1/15\n",
      "98845/98845 [==============================] - 60s - loss: -2.5139 - val_loss: -2.7696\n",
      "Epoch 2/15\n",
      "98845/98845 [==============================] - 57s - loss: -2.7893 - val_loss: -2.8142\n",
      "Epoch 3/15\n",
      "98845/98845 [==============================] - 59s - loss: -2.8170 - val_loss: -2.8319\n",
      "Epoch 4/15\n",
      "98845/98845 [==============================] - 65s - loss: -2.8275 - val_loss: -2.8377\n",
      "Epoch 5/15\n",
      "98845/98845 [==============================] - 69s - loss: -2.8319 - val_loss: -2.8413\n",
      "Epoch 6/15\n",
      "98845/98845 [==============================] - 65s - loss: -2.8348 - val_loss: -2.8437\n",
      "Epoch 7/15\n",
      "98845/98845 [==============================] - 64s - loss: -2.8368 - val_loss: -2.8453\n",
      "Epoch 8/15\n",
      "98845/98845 [==============================] - 66s - loss: -2.8383 - val_loss: -2.8466\n",
      "Epoch 9/15\n",
      "98845/98845 [==============================] - 67s - loss: -2.8394 - val_loss: -2.8475\n",
      "Epoch 10/15\n",
      "98845/98845 [==============================] - 64s - loss: -2.8402 - val_loss: -2.8483\n",
      "Epoch 11/15\n",
      "98845/98845 [==============================] - 64s - loss: -2.8409 - val_loss: -2.8490\n",
      "Epoch 12/15\n",
      "98845/98845 [==============================] - 65s - loss: -2.8415 - val_loss: -2.8494\n",
      "Epoch 13/15\n",
      "98845/98845 [==============================] - 65s - loss: -2.8420 - val_loss: -2.8498\n",
      "Epoch 14/15\n",
      "98845/98845 [==============================] - 65s - loss: -2.8424 - val_loss: -2.8502\n",
      "Epoch 15/15\n",
      "98845/98845 [==============================] - 66s - loss: -2.8428 - val_loss: -2.8506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f967127b4e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                nb_epoch=nb_epoch, batch_size=batch_size, shuffle=True, verbose=1,\n",
    "                validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.save('autoenc_15ep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9920/10000 [============================>.] - ETA: 0s\n",
      "Summary: Loss over the test dataset: 0.12, Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluation = model.evaluate(X_test, Y_Test, verbose=1)\n",
    "print('\\nSummary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from resting_state/post_eeg_tsoy_2504.vhdr...\n",
      "Setting channel info structure...\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "eeg = mne.io.read_raw_brainvision(os.path.join(data_dir, fn)).get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "el = eeg[:, :50]\n",
    "el_sc = scaler.transform(el).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00307421569033914"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(el.reshape(1, -1) - scaler.inverse_transform(autoencoder.predict(el_sc))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00360916 -0.00361387 -0.00361689 -0.00360994 -0.00359949 -0.00358958\n",
      " -0.00358601 -0.00358044 -0.00358398 -0.00358713 -0.00358159 -0.00358638\n",
      " -0.00359578 -0.00359912 -0.00360134 -0.00360818 -0.00360769 -0.0036105\n",
      " -0.00361348 -0.00361128 -0.00360068 -0.00360046 -0.00360051 -0.00358953\n",
      " -0.00357937 -0.00358054 -0.00357468 -0.00356375 -0.00357278 -0.00357795\n",
      " -0.00358372 -0.00359341 -0.00359719 -0.00360134 -0.0036009  -0.00359924\n",
      " -0.00360044 -0.00359529 -0.00359988 -0.00360706 -0.00360305 -0.00360081\n",
      " -0.00359814 -0.00360239 -0.00361304 -0.00361667 -0.00361108 -0.00360125\n",
      " -0.00360303 -0.00360442]\n",
      "[-0.00066897 -0.0006707  -0.00067593 -0.00067092 -0.00065901 -0.00064861\n",
      " -0.00064778 -0.00064666 -0.00064683 -0.00064709 -0.00064106 -0.00064653\n",
      " -0.00066272 -0.00066138 -0.00066204 -0.00066785 -0.0006677  -0.00066646\n",
      " -0.00066841 -0.00066582 -0.00065947 -0.00065989 -0.00066062 -0.00065173\n",
      " -0.00064255 -0.00064285 -0.00064221 -0.00063796 -0.00064412 -0.00064861\n",
      " -0.00065066 -0.00065532 -0.0006615  -0.00066292 -0.00066082 -0.00066335\n",
      " -0.00066284 -0.00065354 -0.00065427 -0.00066174 -0.00066233 -0.00066248\n",
      " -0.00066147 -0.00066733 -0.00067607 -0.00067883 -0.00067524 -0.00067151\n",
      " -0.00067314 -0.00067043]\n",
      "[ 0.00852087  0.00851577  0.00851172  0.00851633  0.00852581  0.00853567\n",
      "  0.00853806  0.00853484  0.0085334   0.00853562  0.0085561   0.00854854\n",
      "  0.00852258  0.00852397  0.00852502  0.00852039  0.00852146  0.00852129\n",
      "  0.00851763  0.00852061  0.00852686  0.00852483  0.00852581  0.00853547\n",
      "  0.00854402  0.00854211  0.00854202  0.0085407   0.00853264  0.00853479\n",
      "  0.00853909  0.00853413  0.00852576  0.00852468  0.00852751  0.00852639\n",
      "  0.00852734  0.00853865  0.00853323  0.00852473  0.00852512  0.00852473\n",
      "  0.00852881  0.0085248   0.00851558  0.00850925  0.00851416  0.00851418\n",
      "  0.00851177  0.00851418]\n",
      "[ 0.00437986  0.00437534  0.00436963  0.00437234  0.0043802   0.00439143\n",
      "  0.00439236  0.00440969  0.00444468  0.00443181  0.0043584   0.00434858\n",
      "  0.00438508  0.00438711  0.00438396  0.00437795  0.00438259  0.00437959\n",
      "  0.00437417  0.00437549  0.00438315  0.00437917  0.00437898  0.00439036\n",
      "  0.00439802  0.00439592  0.00439885  0.0044012   0.00439631  0.0043946\n",
      "  0.00439275  0.00438989  0.00438406  0.00438191  0.00438489  0.00438528\n",
      "  0.00438555  0.00439714  0.00439329  0.0043825   0.00438176  0.00438215\n",
      "  0.00438599  0.00438203  0.00437231  0.00436785  0.00436924  0.00437087\n",
      "  0.00437227  0.00437415]\n",
      "[ 0.00072966  0.00072285  0.00072212  0.00072639  0.00073167  0.00074285\n",
      "  0.00074841  0.00074729  0.00074736  0.00074709  0.00074697  0.000746\n",
      "  0.00074377  0.00074165  0.00073992  0.00073289  0.00073054  0.00074058\n",
      "  0.00073848  0.00072808  0.00072778  0.00073015  0.0007303   0.00073997\n",
      "  0.00074988  0.00074851  0.00075432  0.00075681  0.00075115  0.0007489\n",
      "  0.00074612  0.0007436   0.00073997  0.00073538  0.00073674  0.00073435\n",
      "  0.00073574  0.00074993  0.0007447   0.00073308  0.00073318  0.00073499\n",
      "  0.00073848  0.00073613  0.00072856  0.00072366  0.00072275  0.00072097\n",
      "  0.00072349  0.00072649]\n",
      "[ 0.00746306  0.0074564   0.00745288  0.00745491  0.00745945  0.0074678\n",
      "  0.00746936  0.00746257  0.00745474  0.00745745  0.007475    0.00748843\n",
      "  0.00747771  0.00746958  0.00746443  0.00745964  0.00746121  0.0074606\n",
      "  0.00746245  0.00746924  0.00747427  0.00747263  0.00746907  0.00747488\n",
      "  0.00747844  0.00747434  0.00747537  0.00747454  0.00747034  0.00746672\n",
      "  0.00746516  0.00746309  0.00745745  0.00745447  0.00745886  0.0074613\n",
      "  0.00746523  0.00747883  0.00747419  0.00746458  0.00746228  0.00746169\n",
      "  0.00746699  0.00746572  0.0074585   0.00745188  0.00745142  0.00745066\n",
      "  0.00745068  0.00745203]\n",
      "[ 0.01862922  0.01862388  0.01861919  0.01862497  0.01863347  0.01864197\n",
      "  0.01864392  0.01864377  0.01864585  0.01864709  0.01865718  0.01864844\n",
      "  0.01862869  0.01863037  0.01863035  0.01862566  0.01862678  0.01862979\n",
      "  0.01862705  0.01862852  0.01863428  0.01863406  0.0186345   0.01864343\n",
      "  0.01865168  0.01864744  0.01864748  0.01865051  0.0186446   0.01864119\n",
      "  0.01863923  0.01863555  0.01862878  0.01862966  0.01863269  0.01863225\n",
      "  0.01863418  0.01864285  0.01863904  0.01863176  0.01863137  0.01863088\n",
      "  0.01863188  0.01862729  0.0186186   0.01861423  0.01861406  0.01861697\n",
      "  0.01861973  0.01862239]\n",
      "[ 0.0148678   0.01486284  0.01485698  0.01485701  0.01486516  0.014874\n",
      "  0.01487507  0.01487415  0.01487324  0.01487546  0.01488721  0.01488271\n",
      "  0.01486448  0.01486406  0.01486492  0.01486204  0.01486248  0.01486621\n",
      "  0.01486147  0.01486084  0.0148667   0.01486667  0.01486863  0.0148782\n",
      "  0.01488206  0.01487849  0.01487927  0.01488035  0.01487517  0.01487014\n",
      "  0.01486917  0.01486562  0.0148616   0.01486165  0.01486448  0.01486482\n",
      "  0.01486855  0.01488152  0.01487415  0.01486501  0.01486672  0.0148679\n",
      "  0.01486812  0.01486318  0.01485552  0.01485105  0.01485371  0.01485476\n",
      "  0.01485435  0.01485486]\n",
      "[-0.00355569 -0.00355933 -0.00356172 -0.00355906 -0.00355872 -0.0035532\n",
      " -0.00355154 -0.0035531  -0.00355264 -0.0035449  -0.00353989 -0.00353682\n",
      " -0.00354246 -0.00355161 -0.0035614  -0.00356692 -0.00356624 -0.00356528\n",
      " -0.00356538 -0.00356084 -0.00355317 -0.00354978 -0.00354822 -0.00354636\n",
      " -0.00354368 -0.00354392 -0.00354128 -0.00354158 -0.00354749 -0.00354734\n",
      " -0.00354875 -0.00355288 -0.00355608 -0.00356194 -0.00356531 -0.00356169\n",
      " -0.00355649 -0.00353586 -0.00353708 -0.00354617 -0.00354861 -0.00354836\n",
      " -0.00354673 -0.00354995 -0.00356035 -0.00356682 -0.00356528 -0.00356677\n",
      " -0.00356782 -0.0035613 ]\n",
      "[ 0.0096802   0.00967505  0.00966807  0.00966711  0.0096759   0.0096825\n",
      "  0.00968289  0.00967844  0.00967988  0.00968274  0.00968574  0.00968398\n",
      "  0.00967644  0.00967319  0.00967141  0.00966838  0.00967     0.00967036\n",
      "  0.00966748  0.00966831  0.00967324  0.00967742  0.00968     0.0096843\n",
      "  0.00968967  0.00968831  0.00968508  0.00968372  0.00967903  0.00967527\n",
      "  0.00967498  0.0096772   0.00967341  0.00967322  0.00967502  0.00967673\n",
      "  0.0096793   0.00968909  0.00968748  0.00967883  0.00967576  0.0096741\n",
      "  0.00967744  0.0096752   0.0096687   0.00966555  0.00966826  0.00966726\n",
      "  0.00966763  0.00967087]\n",
      "[-0.00193137 -0.00193755 -0.00194097 -0.00193923 -0.00192971 -0.00192068\n",
      " -0.00192043 -0.00192258 -0.00192026 -0.00192092 -0.00191621 -0.00192051\n",
      " -0.00193298 -0.00193169 -0.00192976 -0.00193542 -0.00193591 -0.00193398\n",
      " -0.00193662 -0.00193335 -0.00192729 -0.00192737 -0.00192849 -0.00191848\n",
      " -0.00191384 -0.00191731 -0.00191931 -0.00191829 -0.00192346 -0.00192737\n",
      " -0.0019281  -0.00192939 -0.00193328 -0.0019321  -0.00192966 -0.00192966\n",
      " -0.00192793 -0.00191858 -0.00192205 -0.00192825 -0.00192971 -0.00192917\n",
      " -0.0019292  -0.00193384 -0.00194282 -0.00194451 -0.00194004 -0.00193845\n",
      " -0.00193999 -0.00193982]\n",
      "[ 0.00097126  0.00096387  0.00095845  0.00096213  0.00097136  0.00098171\n",
      "  0.00098293  0.00098323  0.00098403  0.00098435  0.00098555  0.00097944\n",
      "  0.00097131  0.00097261  0.00097253  0.00096506  0.00096599  0.00097192\n",
      "  0.0009708   0.00097305  0.00097925  0.00097883  0.00097615  0.00098386\n",
      "  0.00099124  0.0009843   0.00098318  0.00098572  0.00098269  0.00097678\n",
      "  0.00097488  0.00097451  0.00097205  0.00097292  0.000974    0.00097283\n",
      "  0.00097671  0.00098523  0.00098176  0.00097659  0.00097512  0.00097156\n",
      "  0.00097236  0.00096826  0.00095977  0.00095823  0.00096064  0.00096018\n",
      "  0.00096255  0.0009657 ]\n",
      "[-0.00416741 -0.00417314 -0.00418037 -0.00417898 -0.00417039 -0.0041613\n",
      " -0.00416023 -0.00416335 -0.0041624  -0.00416018 -0.00415845 -0.00416045\n",
      " -0.00417136 -0.00417251 -0.00417273 -0.00417444 -0.0041741  -0.00417368\n",
      " -0.00417598 -0.00417324 -0.00416816 -0.00416497 -0.00416428 -0.00415725\n",
      " -0.00415271 -0.00415759 -0.00416145 -0.00416409 -0.00416726 -0.00417034\n",
      " -0.00416946 -0.0041689  -0.00417224 -0.00417092 -0.00416936 -0.00417024\n",
      " -0.00416689 -0.00415461 -0.00415916 -0.00416907 -0.00416897 -0.00417073\n",
      " -0.00417046 -0.00417466 -0.00418032 -0.00418132 -0.00417603 -0.00417664\n",
      " -0.00417573 -0.00417434]\n",
      "[ 0.00311394  0.00310957  0.00309951  0.0030988   0.00310491  0.0031113\n",
      "  0.00311311  0.00311008  0.00310952  0.00311541  0.00311392  0.0031103\n",
      "  0.00310515  0.00310332  0.00310334  0.00309954  0.00309963  0.00310332\n",
      "  0.0030999   0.0031021   0.00310923  0.0031136   0.00311833  0.00312205\n",
      "  0.00312209  0.00311653  0.00311135  0.00310383  0.00309993  0.00309817\n",
      "  0.00309983  0.00310361  0.00310461  0.00310447  0.00310623  0.00310999\n",
      "  0.0031123   0.00312083  0.00311868  0.0031115   0.00310608  0.00310178\n",
      "  0.00310303  0.00309927  0.00309883  0.00309963  0.00310483  0.00310422\n",
      "  0.00310532  0.00310667]\n",
      "[-0.00976902 -0.00978208 -0.00979043 -0.00978733 -0.00977698 -0.0097637\n",
      " -0.00976018 -0.00976331 -0.00976289 -0.00976208 -0.00976309 -0.00977007\n",
      " -0.0097781  -0.00977852 -0.00977922 -0.00978352 -0.00978079 -0.00977612\n",
      " -0.00977749 -0.00977646 -0.00977183 -0.00976633 -0.00976521 -0.0097595\n",
      " -0.00975608 -0.00976453 -0.00976956 -0.00977053 -0.00977214 -0.0097761\n",
      " -0.00977449 -0.00977461 -0.00977639 -0.00977332 -0.00977288 -0.00977214\n",
      " -0.00976787 -0.00975847 -0.0097635  -0.00976887 -0.00976912 -0.00977463\n",
      " -0.00977832 -0.00978193 -0.00978428 -0.0097823  -0.00978032 -0.00978083\n",
      " -0.00978159 -0.00977888]\n",
      "[ 0.00299211  0.00298052  0.00297349  0.00297844  0.00299036  0.00300105\n",
      "  0.00300642  0.00300579  0.00300464  0.00300154  0.00299995  0.00299399\n",
      "  0.00298391  0.00298687  0.00298689  0.0029801   0.00297942  0.00298745\n",
      "  0.00298633  0.00299067  0.00299624  0.00299573  0.0029968   0.00300227\n",
      "  0.00300803  0.00300012  0.00299524  0.00299734  0.00299583  0.00299065\n",
      "  0.00298948  0.00298999  0.00298914  0.0029925   0.00299182  0.00299197\n",
      "  0.00299536  0.003005    0.00299998  0.00299626  0.00299656  0.00298967\n",
      "  0.00298477  0.00297734  0.0029731   0.00297576  0.00297891  0.00297751\n",
      "  0.002979    0.00298005]\n",
      "[-0.00834377 -0.00835215 -0.00835884 -0.00835403 -0.0083408  -0.00833167\n",
      " -0.00832825 -0.00832898 -0.00832764 -0.0083325  -0.00833262 -0.00833794\n",
      " -0.00834636 -0.00834121 -0.0083406  -0.00834749 -0.00834724 -0.00834102\n",
      " -0.00834185 -0.00833994 -0.00833477 -0.00833362 -0.00833469 -0.00832898\n",
      " -0.00832151 -0.00832878 -0.00833015 -0.00832659 -0.00832849 -0.00833479\n",
      " -0.00833762 -0.00833848 -0.00833972 -0.00833909 -0.00833728 -0.00833855\n",
      " -0.00833564 -0.00832722 -0.00833147 -0.0083364  -0.00833411 -0.00833982\n",
      " -0.00834502 -0.00834893 -0.00835308 -0.00835637 -0.00835259 -0.00835105\n",
      " -0.00835142 -0.00835081]\n",
      "[ 0.00765945  0.007654    0.00764688  0.007653    0.0076658   0.00767639\n",
      "  0.00767834  0.00767595  0.00767671  0.00767302  0.00767676  0.00767344\n",
      "  0.00766511  0.00766533  0.00766555  0.00765925  0.00765979  0.00766357\n",
      "  0.00766211  0.00766797  0.00767114  0.00766941  0.00766658  0.0076719\n",
      "  0.00768025  0.00767996  0.00767761  0.00768     0.00767703  0.00767078\n",
      "  0.00766804  0.0076667   0.00766326  0.00766619  0.00766736  0.00766545\n",
      "  0.00766626  0.00767585  0.00767019  0.00766687  0.00766917  0.00766692\n",
      "  0.00766572  0.00766074  0.0076521   0.00764983  0.0076542   0.00765627\n",
      "  0.00765664  0.00765583]\n",
      "[ 0.00393103  0.00392383  0.0039189   0.00392585  0.00393811  0.00394709\n",
      "  0.00394841  0.00395027  0.0039501   0.00394666  0.00395029  0.00394507\n",
      "  0.00393757  0.00393853  0.00393704  0.00392805  0.00393108  0.00393848\n",
      "  0.00393545  0.00393794  0.00394277  0.00394036  0.0039385   0.003945\n",
      "  0.0039533   0.00394978  0.00395027  0.00395388  0.00394949  0.00394255\n",
      "  0.00393918  0.00393501  0.00393069  0.0039325   0.00393386  0.00393167\n",
      "  0.00393369  0.00394377  0.00393826  0.00393503  0.00393972  0.00393909\n",
      "  0.0039374   0.00393145  0.00392275  0.00391809  0.0039209   0.00392585\n",
      "  0.00392627  0.00392678]\n",
      "[ -8.86474609e-05  -9.35058594e-05  -9.94628906e-05  -9.30419922e-05\n",
      "  -8.26904297e-05  -7.35595703e-05  -7.18994141e-05  -7.18505859e-05\n",
      "  -7.08007812e-05  -7.28759766e-05  -6.98242187e-05  -7.40722656e-05\n",
      "  -8.22998047e-05  -8.24707031e-05  -8.04931641e-05  -8.90380859e-05\n",
      "  -8.85498047e-05  -8.16406250e-05  -8.59863281e-05  -8.29101562e-05\n",
      "  -7.68066406e-05  -7.67333984e-05  -7.95166016e-05  -7.29248047e-05\n",
      "  -6.71142578e-05  -7.09228516e-05  -6.94091797e-05  -6.69677734e-05\n",
      "  -7.15576172e-05  -7.60986328e-05  -7.95166016e-05  -8.02246094e-05\n",
      "  -8.29345703e-05  -8.32275391e-05  -8.12744141e-05  -8.14697266e-05\n",
      "  -8.19335938e-05  -7.28759766e-05  -7.68798828e-05  -8.09326172e-05\n",
      "  -7.75146484e-05  -7.92236328e-05  -8.13476562e-05  -8.70605469e-05\n",
      "  -9.46777344e-05  -1.00366211e-04  -9.42382813e-05  -9.25048828e-05\n",
      "  -9.14550781e-05  -9.13330078e-05]\n",
      "[ 0.00096741  0.00096226  0.00095654  0.00095901  0.0009697   0.00098103\n",
      "  0.00098396  0.00098367  0.00098481  0.00098367  0.00098765  0.0009814\n",
      "  0.00097034  0.00097241  0.00097205  0.00096472  0.00096658  0.00096929\n",
      "  0.00096533  0.00096895  0.00097788  0.00097673  0.00097419  0.00098181\n",
      "  0.00098977  0.00098679  0.00098708  0.00099021  0.00098381  0.00097791\n",
      "  0.00097571  0.00097246  0.00096921  0.00097195  0.00097166  0.00097161\n",
      "  0.00097354  0.00098215  0.00097693  0.00097146  0.00097244  0.00097151\n",
      "  0.0009709   0.00096514  0.00095493  0.00095266  0.00095591  0.00095754\n",
      "  0.00095894  0.00095803]\n",
      "[ 0.00018215  0.0001769   0.00017266  0.00017703  0.0001884   0.00019832\n",
      "  0.00020081  0.0002009   0.00020234  0.00020002  0.00020527  0.00020205\n",
      "  0.00019348  0.00019438  0.00019485  0.00018669  0.00018752  0.00019307\n",
      "  0.00018945  0.00019175  0.00019688  0.00019592  0.00019333  0.00019792\n",
      "  0.00020525  0.00020437  0.00020623  0.00020823  0.00020583  0.000197\n",
      "  0.00019558  0.00019331  0.00019031  0.00019177  0.00019202  0.00019011\n",
      "  0.00019014  0.00020144  0.00019705  0.00019353  0.00019558  0.00019441\n",
      "  0.00019375  0.00018521  0.00017764  0.0001717   0.00017783  0.00018225\n",
      "  0.00018271  0.00018167]\n",
      "[ 0.00204851  0.00204331  0.00204009  0.00204402  0.0020553   0.00205935\n",
      "  0.00205798  0.00205823  0.00206265  0.00206384  0.00206758  0.00206406\n",
      "  0.00205598  0.00205923  0.00205862  0.00204954  0.00204827  0.00205107\n",
      "  0.0020394   0.00204189  0.00205142  0.00205388  0.00205774  0.00207478\n",
      "  0.00208323  0.0020739   0.00207146  0.00207629  0.00207009  0.00206296\n",
      "  0.00206135  0.00205918  0.00205549  0.00205476  0.00205559  0.00205037\n",
      "  0.00204946  0.00206213  0.00205857  0.00205261  0.00205496  0.00205374\n",
      "  0.00205601  0.00205083  0.00203979  0.0020364   0.00204312  0.00204758\n",
      "  0.00204868  0.00204978]\n",
      "[-0.00539641 -0.00540796 -0.00541313 -0.00540559 -0.00539333 -0.00538582\n",
      " -0.0053865  -0.00538352 -0.0053813  -0.00538577 -0.00537715 -0.00537827\n",
      " -0.00538464 -0.00538506 -0.00538547 -0.00539753 -0.00539988 -0.00539321\n",
      " -0.00539634 -0.00539468 -0.00539409 -0.00539441 -0.00539709 -0.0053866\n",
      " -0.005376   -0.00537419 -0.00537263 -0.00536702 -0.00537234 -0.00538064\n",
      " -0.00538289 -0.00538755 -0.00539431 -0.0053947  -0.00539319 -0.00539685\n",
      " -0.00539409 -0.0053821  -0.00538674 -0.00539504 -0.00539265 -0.00539231\n",
      " -0.00539443 -0.00539863 -0.00540947 -0.00541399 -0.00540703 -0.00540134\n",
      " -0.00540249 -0.00540129]\n",
      "[ 0.00169436  0.00168804  0.00168228  0.0016886   0.00170027  0.00171038\n",
      "  0.00170974  0.00171067  0.00171162  0.00170852  0.00171421  0.00170991\n",
      "  0.00170359  0.00170752  0.00170632  0.0016949   0.00169265  0.00170005\n",
      "  0.00169883  0.001702    0.00170732  0.0017053   0.00170247  0.00170964\n",
      "  0.00171926  0.00171506  0.00171785  0.00172214  0.00171721  0.00171091\n",
      "  0.00170881  0.00170405  0.00170022  0.00170271  0.00170364  0.00169885\n",
      "  0.00169995  0.00171052  0.00170496  0.00170046  0.0017033   0.0017009\n",
      "  0.00170308  0.00169673  0.00168599  0.00168289  0.00168774  0.00169426\n",
      "  0.00169644  0.00169587]\n",
      "[ 0.0062283   0.00622271  0.00621812  0.00622224  0.00623425  0.00624246\n",
      "  0.00624363  0.0062467   0.00624663  0.00624495  0.00625098  0.00624556\n",
      "  0.00623533  0.00623613  0.00623523  0.00622942  0.00622878  0.00623184\n",
      "  0.00622861  0.00623223  0.00623872  0.00623923  0.0062364   0.00624387\n",
      "  0.00625139  0.00624875  0.00625095  0.00625579  0.00625046  0.00624226\n",
      "  0.0062406   0.00623633  0.00623293  0.00623455  0.00623772  0.00623547\n",
      "  0.00623413  0.00624387  0.0062387   0.0062345   0.00623743  0.00623679\n",
      "  0.00623569  0.00622817  0.00621782  0.00621379  0.00621846  0.0062238\n",
      "  0.0062248   0.00622581]\n",
      "[ 0.00595735  0.00595205  0.00594614  0.00595247  0.00596423  0.00597517\n",
      "  0.0059762   0.00597605  0.00597593  0.00597498  0.00597954  0.00597661\n",
      "  0.00596746  0.00597104  0.00596926  0.00595969  0.00595994  0.00596191\n",
      "  0.00595942  0.00596494  0.00597046  0.00596921  0.0059657   0.00597292\n",
      "  0.00598162  0.00598118  0.00598381  0.00598694  0.00598118  0.00597395\n",
      "  0.00597024  0.0059668   0.00596418  0.00596477  0.00596804  0.00596487\n",
      "  0.00596406  0.00597258  0.00597009  0.00596404  0.00596702  0.00596643\n",
      "  0.00596567  0.00596021  0.00594956  0.00594519  0.00595146  0.00595715\n",
      "  0.0059585   0.00595876]\n",
      "[-0.00273606 -0.00274233 -0.00274907 -0.00274167 -0.00272849 -0.00272068\n",
      " -0.00271809 -0.00271335 -0.00271436 -0.00271843 -0.00271582 -0.00272031\n",
      " -0.00272673 -0.00272354 -0.00272322 -0.00273386 -0.00273938 -0.00273853\n",
      " -0.0027415  -0.00273633 -0.00272974 -0.00273079 -0.00272981 -0.00271838\n",
      " -0.00270735 -0.0027074  -0.00270291 -0.00269338 -0.00270173 -0.00271121\n",
      " -0.0027176  -0.00272583 -0.00273167 -0.00273435 -0.00273235 -0.00273298\n",
      " -0.00273252 -0.00272585 -0.00272771 -0.0027342  -0.00273069 -0.00272922\n",
      " -0.00272739 -0.00273423 -0.00274629 -0.00274856 -0.00273867 -0.00272781\n",
      " -0.00272881 -0.00273083]\n",
      "[ 0.0075238   0.00751763  0.00751162  0.00751692  0.00752502  0.00753538\n",
      "  0.00753738  0.00753645  0.00753301  0.00752986  0.00753271  0.00753398\n",
      "  0.00753381  0.00753403  0.00753157  0.00752668  0.00753201  0.00752798\n",
      "  0.00751685  0.00752007  0.00753008  0.00752634  0.00752375  0.00753215\n",
      "  0.00754231  0.00754172  0.00754607  0.00755061  0.00754417  0.00754089\n",
      "  0.00753738  0.00753159  0.00752986  0.00752644  0.00752761  0.00752795\n",
      "  0.00752954  0.00754006  0.0075364   0.00752756  0.00752805  0.00752942\n",
      "  0.00753013  0.00752368  0.00751455  0.00751057  0.0075127   0.00751394\n",
      "  0.00751523  0.0075158 ]\n",
      "[-0.00089226 -0.00089512 -0.00090073 -0.00089905 -0.00088728 -0.00087546\n",
      " -0.00087156 -0.00087141 -0.00087544 -0.00087727 -0.00087065 -0.00087686\n",
      " -0.00088728 -0.00088794 -0.00088933 -0.00089382 -0.00089343 -0.00089492\n",
      " -0.00090029 -0.00089497 -0.00088589 -0.0008864  -0.00088757 -0.00087864\n",
      " -0.00086965 -0.00087166 -0.00086633 -0.0008573  -0.00086433 -0.00087371\n",
      " -0.00087825 -0.00088467 -0.00088923 -0.00089119 -0.00088943 -0.00088889\n",
      " -0.00089092 -0.00088259 -0.00088494 -0.00089084 -0.00089026 -0.00088987\n",
      " -0.00088745 -0.00089072 -0.00090073 -0.000903   -0.00089771 -0.00089387\n",
      " -0.00089668 -0.00089641]\n",
      "[ 0.00048997  0.0004856   0.0004813   0.00048792  0.00049817  0.00050637\n",
      "  0.00051003  0.00051409  0.0005126   0.00050667  0.00050991  0.00050503\n",
      "  0.00049431  0.00049487  0.00049514  0.00048835  0.00048552  0.00048281\n",
      "  0.00048291  0.00048794  0.00049546  0.00049431  0.00049236  0.00050369\n",
      "  0.00051194  0.00051311  0.00052141  0.00052874  0.00052073  0.00051145\n",
      "  0.00050525  0.00050063  0.00049524  0.00049402  0.00049485  0.00049368\n",
      "  0.00049053  0.00049358  0.00049192  0.00048748  0.00049109  0.00049089\n",
      "  0.00049321  0.00048877  0.00047905  0.00047478  0.00048252  0.00049182\n",
      "  0.00049014  0.0004885 ]\n",
      "[-0.00038406 -0.00038872 -0.0003939  -0.00039089 -0.00038162 -0.00036897\n",
      " -0.00036687 -0.00036946 -0.00036885 -0.00036794 -0.00036011 -0.0003688\n",
      " -0.00038342 -0.00038018 -0.00038049 -0.00038792 -0.00038591 -0.00038364\n",
      " -0.00038726 -0.00038501 -0.00037764 -0.00037971 -0.00038044 -0.00037229\n",
      " -0.00036277 -0.00036467 -0.00036101 -0.00035862 -0.00036448 -0.00037166\n",
      " -0.00037351 -0.00037852 -0.00038191 -0.00038372 -0.00038079 -0.00038171\n",
      " -0.00038096 -0.00037048 -0.00037527 -0.00038171 -0.00037971 -0.00037927\n",
      " -0.00038076 -0.00038735 -0.00039687 -0.00039856 -0.00039375 -0.00039285\n",
      " -0.00039268 -0.00039387]\n",
      "[ 0.0049446   0.00493809  0.00493257  0.00493503  0.00494563  0.00495486\n",
      "  0.00495784  0.00494944  0.00493477  0.0049386   0.0049731   0.00497534\n",
      "  0.00495105  0.00494961  0.00495085  0.00494602  0.00494622  0.00494448\n",
      "  0.00494194  0.00494556  0.00495347  0.00494846  0.004945    0.00495471\n",
      "  0.0049637   0.00496072  0.00496165  0.00495999  0.0049512   0.0049509\n",
      "  0.00495466  0.00494902  0.00494626  0.00494646  0.00494861  0.00494714\n",
      "  0.0049478   0.00495886  0.0049554   0.00494934  0.00495002  0.00494763\n",
      "  0.00494873  0.00494395  0.00493384  0.00493044  0.00493584  0.00493269\n",
      "  0.00493115  0.00493352]\n",
      "[ 0.0009678   0.00096284  0.00095098  0.00095115  0.00095789  0.0009658\n",
      "  0.00096926  0.00096355  0.00095386  0.00095554  0.00097129  0.00098545\n",
      "  0.0009761   0.00096816  0.00096462  0.00095876  0.00096062  0.00096279\n",
      "  0.00096426  0.00097217  0.00097881  0.00097673  0.00097024  0.0009759\n",
      "  0.0009781   0.00097034  0.00097151  0.00097493  0.0009698   0.00096292\n",
      "  0.00096067  0.00095591  0.00095452  0.00095593  0.00095984  0.00096287\n",
      "  0.00096626  0.00097991  0.00097595  0.00096775  0.00096594  0.00096731\n",
      "  0.00096748  0.00096338  0.00095635  0.00094768  0.00094844  0.00094797\n",
      "  0.00094751  0.00094792]\n",
      "[-0.00324231 -0.00324849 -0.00325488 -0.003253   -0.00324436 -0.00323386\n",
      " -0.00323333 -0.00323513 -0.00322949 -0.00322668 -0.0032085  -0.00322412\n",
      " -0.0032551  -0.003248   -0.00324177 -0.0032469  -0.00324895 -0.00324531\n",
      " -0.00324814 -0.00324492 -0.00323765 -0.0032386  -0.00324094 -0.00323435\n",
      " -0.0032283  -0.00322961 -0.00322791 -0.00322825 -0.00323367 -0.00323806\n",
      " -0.00324036 -0.00324409 -0.00324612 -0.0032467  -0.00324407 -0.00324392\n",
      " -0.00324365 -0.00322932 -0.00323376 -0.00324265 -0.00324138 -0.00324163\n",
      " -0.00324419 -0.00324805 -0.00325684 -0.00326155 -0.00326021 -0.00325793\n",
      " -0.00325903 -0.00325959]\n",
      "[-0.00132483 -0.00132983 -0.00133594 -0.0013323  -0.00132024 -0.0013113\n",
      " -0.00131023 -0.00131028 -0.00131084 -0.00131301 -0.00130747 -0.00131182\n",
      " -0.00132371 -0.00132095 -0.00132141 -0.0013282  -0.00132688 -0.00132437\n",
      " -0.00132632 -0.00132397 -0.00131768 -0.0013175  -0.00132019 -0.0013134\n",
      " -0.0013052  -0.0013071  -0.00130667 -0.00130408 -0.00131028 -0.00131653\n",
      " -0.00131814 -0.00132104 -0.001324   -0.00132263 -0.00132014 -0.00131995\n",
      " -0.00131924 -0.00131052 -0.00131409 -0.0013198  -0.00131882 -0.00131873\n",
      " -0.00132178 -0.00132949 -0.00133916 -0.00134421 -0.0013377  -0.00133459\n",
      " -0.0013373  -0.00133601]\n",
      "[ 0.00122014  0.00121377  0.00120762  0.00121062  0.00121838  0.00122659\n",
      "  0.00122976  0.00122922  0.00122842  0.00122727  0.00123545  0.00123013\n",
      "  0.00121907  0.00122021  0.00122131  0.00121531  0.00121379  0.00121992\n",
      "  0.0012165   0.0012189   0.00122505  0.00122351  0.00122219  0.00122976\n",
      "  0.00123616  0.00123396  0.00123269  0.00123562  0.00123044  0.00122283\n",
      "  0.00122336  0.00122017  0.00121804  0.00122009  0.00122131  0.00122087\n",
      "  0.00122129  0.00123406  0.0012302   0.00122488  0.00122483  0.00122283\n",
      "  0.00121968  0.0012125   0.00120469  0.00120066  0.00120444  0.00120686\n",
      "  0.00120547  0.00120681]\n",
      "[ 0.00519895  0.00519277  0.00518579  0.00518494  0.00518982  0.00519856\n",
      "  0.00519915  0.0051946   0.00519312  0.00519563  0.00520459  0.00520728\n",
      "  0.0052009   0.00519707  0.00519421  0.00519001  0.0051926   0.00519321\n",
      "  0.00519058  0.00519175  0.00519878  0.00519924  0.00519753  0.00520066\n",
      "  0.00520471  0.00520383  0.00520427  0.00520403  0.00519963  0.00519324\n",
      "  0.00519031  0.0051874   0.00518479  0.00518523  0.00519011  0.00519309\n",
      "  0.00519756  0.00520979  0.00520574  0.00519915  0.00519827  0.00519583\n",
      "  0.00519751  0.00519541  0.00518687  0.00518215  0.00518286  0.0051824\n",
      "  0.00518125  0.00518127]\n",
      "[-0.00093025 -0.00093364 -0.00094282 -0.00094587 -0.00094436 -0.0009386\n",
      " -0.00093845 -0.00094421 -0.00094551 -0.00094084 -0.0009394  -0.00093813\n",
      " -0.00094211 -0.00094082 -0.00094153 -0.00094504 -0.00094348 -0.00094209\n",
      " -0.00094497 -0.00094458 -0.00094009 -0.00093679 -0.00093323 -0.00092859\n",
      " -0.00092542 -0.00092903 -0.00092991 -0.0009345  -0.0009408  -0.0009488\n",
      " -0.00094895 -0.00094365 -0.00094041 -0.00094167 -0.00094187 -0.0009386\n",
      " -0.00093501 -0.00092224 -0.00092659 -0.00093748 -0.00094133 -0.00094392\n",
      " -0.00094219 -0.00094438 -0.00094756 -0.00094714 -0.0009439  -0.00094343\n",
      " -0.00094463 -0.00094373]\n",
      "[ 0.00016331  0.00015518  0.00014717  0.00014636  0.00015598  0.00016741\n",
      "  0.00016682  0.00016487  0.00016567  0.00016531  0.00017129  0.00016738\n",
      "  0.00015657  0.00015625  0.00015725  0.00015359  0.00015481  0.00015649\n",
      "  0.00015322  0.0001564   0.00016362  0.00016414  0.00016111  0.00016741\n",
      "  0.00017424  0.00017122  0.00016956  0.00017102  0.00016497  0.00016003\n",
      "  0.00015793  0.00015752  0.00015466  0.00015623  0.00015798  0.0001593\n",
      "  0.00016113  0.00017107  0.0001679   0.00016067  0.00016077  0.00015906\n",
      "  0.00015698  0.00015283  0.00014487  0.0001427   0.000146    0.00014568\n",
      "  0.00014663  0.00014583]\n",
      "[-0.01296042 -0.01296772 -0.01297544 -0.01297478 -0.01296394 -0.01295364\n",
      " -0.01295002 -0.01295481 -0.01295449 -0.01295442 -0.01295303 -0.01295532\n",
      " -0.01296521 -0.01296333 -0.01296238 -0.01296814 -0.01296907 -0.01296187\n",
      " -0.01296392 -0.01296182 -0.01295664 -0.01295774 -0.01295852 -0.01295115\n",
      " -0.01294744 -0.01295012 -0.01295115 -0.01295408 -0.01295798 -0.01296277\n",
      " -0.01296404 -0.01296362 -0.01296335 -0.01296184 -0.01295935 -0.01295999\n",
      " -0.01295825 -0.01294622 -0.01295188 -0.01295642 -0.01295818 -0.01296101\n",
      " -0.01296372 -0.01296992 -0.01297539 -0.01297864 -0.01297319 -0.01297136\n",
      " -0.01297266 -0.01297122]\n",
      "[ 0.00122981  0.00122061  0.00121113  0.00121145  0.00121975  0.00123132\n",
      "  0.00123137  0.00122566  0.00122837  0.00122991  0.00123198  0.00122788\n",
      "  0.00122034  0.00122222  0.00122122  0.00121633  0.00121775  0.00122324\n",
      "  0.0012208   0.00122295  0.00122749  0.00122913  0.00122795  0.00123367\n",
      "  0.00123767  0.00123503  0.00123093  0.00122786  0.00122322  0.00121946\n",
      "  0.00121941  0.00122095  0.00122029  0.00121917  0.00122146  0.00122209\n",
      "  0.00122559  0.00123601  0.00123303  0.00122761  0.00122615  0.00122258\n",
      "  0.00122231  0.00121958  0.00121025  0.00120876  0.00121523  0.00121326\n",
      "  0.00121318  0.00121301]\n",
      "[ 0.00695876  0.00694727  0.00693423  0.00693396  0.00694353  0.00695598\n",
      "  0.00695891  0.00695364  0.00695596  0.00695906  0.00695552  0.00694995\n",
      "  0.00694368  0.00694282  0.0069425   0.00693958  0.00694294  0.00694707\n",
      "  0.0069457   0.00694673  0.00695322  0.00695867  0.00695823  0.00696462\n",
      "  0.00696628  0.00695823  0.00694973  0.00694563  0.00694299  0.00694089\n",
      "  0.00694104  0.00694229  0.00694646  0.00694763  0.00694973  0.00695129\n",
      "  0.00695562  0.00696599  0.00696174  0.00695764  0.006953    0.00694753\n",
      "  0.00694341  0.00693784  0.00693442  0.00693904  0.00694492  0.00694182\n",
      "  0.00694224  0.00694299]\n",
      "[-0.00850022 -0.00851055 -0.0085207  -0.00851902 -0.00850872 -0.00849885\n",
      " -0.00849587 -0.00849978 -0.00849785 -0.00849778 -0.00849971 -0.00850586\n",
      " -0.0085114  -0.00851118 -0.00851125 -0.00851545 -0.00851472 -0.00851006\n",
      " -0.00850942 -0.00850674 -0.00850225 -0.00850042 -0.00850042 -0.00849368\n",
      " -0.00849041 -0.00849812 -0.00850071 -0.00850266 -0.00850789 -0.00851179\n",
      " -0.0085116  -0.00851064 -0.00851013 -0.00850798 -0.00850598 -0.00850564\n",
      " -0.00850166 -0.00849158 -0.00849363 -0.00849802 -0.00850076 -0.00850559\n",
      " -0.00850991 -0.00851445 -0.00851807 -0.00851877 -0.0085146  -0.00851731\n",
      " -0.00851807 -0.00851829]\n",
      "[-0.00932083 -0.00932925 -0.00933892 -0.00933684 -0.00932751 -0.00931663\n",
      " -0.00931389 -0.00931677 -0.00931587 -0.0093179  -0.00931675 -0.00931997\n",
      " -0.0093262  -0.00932363 -0.00932253 -0.00932878 -0.00933064 -0.00932725\n",
      " -0.00932734 -0.00931963 -0.00931543 -0.00931741 -0.00932195 -0.00931482\n",
      " -0.0093092  -0.00931418 -0.00931345 -0.00931179 -0.0093176  -0.00932234\n",
      " -0.00932458 -0.00932666 -0.00932693 -0.00932424 -0.00932351 -0.00932468\n",
      " -0.00932334 -0.00931296 -0.00931482 -0.00931804 -0.00931599 -0.00932\n",
      " -0.00932695 -0.00933311 -0.00933916 -0.00934089 -0.00933545 -0.00933606\n",
      " -0.00933423 -0.00933464]\n",
      "[ 0.00147283  0.00146353  0.00145664  0.00145818  0.00146921  0.00148127\n",
      "  0.00148362  0.00148132  0.00148086  0.00147542  0.00147656  0.00147295\n",
      "  0.00146858  0.00147256  0.00147351  0.00146487  0.00146516  0.00147261\n",
      "  0.0014709   0.0014771   0.00148252  0.00147996  0.00147449  0.00147878\n",
      "  0.00148547  0.00148113  0.00148367  0.00148425  0.00147947  0.00147161\n",
      "  0.0014696   0.00147051  0.00147009  0.00147273  0.00147561  0.00147234\n",
      "  0.00147295  0.00148245  0.00147761  0.00147615  0.00147947  0.00147522\n",
      "  0.00146724  0.00146055  0.00145396  0.0014512   0.00145762  0.00145925\n",
      "  0.00145898  0.00145872]\n",
      "[ 0.00752444  0.00751733  0.00750991  0.00751492  0.00752468  0.0075344\n",
      "  0.00753752  0.00753655  0.0075353   0.00753127  0.00753506  0.00753018\n",
      "  0.00752532  0.00752788  0.00752878  0.00752146  0.00752117  0.00752817\n",
      "  0.00752666  0.0075313   0.00753721  0.00753445  0.00752957  0.00753513\n",
      "  0.0075428   0.00753909  0.00753918  0.00754055  0.0075343   0.00752639\n",
      "  0.00752493  0.00752451  0.00752625  0.00752913  0.0075301   0.0075262\n",
      "  0.00752651  0.0075343   0.00752981  0.00752883  0.0075325   0.00752996\n",
      "  0.00752388  0.00751729  0.00750825  0.00750535  0.00750942  0.00751028\n",
      "  0.00751182  0.00751174]\n",
      "[-0.00612717 -0.00613506 -0.00614243 -0.00613879 -0.00612634 -0.00611619\n",
      " -0.00611301 -0.00611379 -0.00611523 -0.00612131 -0.00611904 -0.00612158\n",
      " -0.00612488 -0.00612114 -0.0061219  -0.00613005 -0.00613049 -0.00612358\n",
      " -0.00612373 -0.00611987 -0.00611548 -0.0061178  -0.0061238  -0.00611829\n",
      " -0.00611008 -0.00611379 -0.00611321 -0.00610999 -0.00611492 -0.00612229\n",
      " -0.00612708 -0.00612827 -0.00612834 -0.00612556 -0.00612366 -0.00612737\n",
      " -0.00612832 -0.00611726 -0.00612112 -0.0061199  -0.00611536 -0.00611829\n",
      " -0.0061249  -0.00613271 -0.00614146 -0.00614319 -0.00613608 -0.00613679\n",
      " -0.00613774 -0.00613821]\n",
      "[-0.00239919 -0.00240693 -0.00241479 -0.00241296 -0.00240427 -0.0023928\n",
      " -0.00238718 -0.00239187 -0.00239106 -0.00239373 -0.00239297 -0.00239712\n",
      " -0.0024009  -0.00239839 -0.00239822 -0.00240647 -0.00240642 -0.00240034\n",
      " -0.0024021  -0.00239551 -0.00239067 -0.00239241 -0.00239607 -0.00239265\n",
      " -0.00238582 -0.00238977 -0.00238845 -0.00238674 -0.00239104 -0.00239915\n",
      " -0.00240188 -0.00240142 -0.00240203 -0.00239944 -0.00239685 -0.00239763\n",
      " -0.00239697 -0.00238972 -0.00239236 -0.00239431 -0.0023906  -0.00239392\n",
      " -0.00240142 -0.00240889 -0.00241577 -0.00241755 -0.00241289 -0.00241248\n",
      " -0.00241338 -0.00241335]\n",
      "[-0.00426404 -0.00426958 -0.00427993 -0.00427717 -0.00426858 -0.00425906\n",
      " -0.00425505 -0.00425837 -0.0042603  -0.00426189 -0.00425859 -0.00426079\n",
      " -0.00426648 -0.00426567 -0.00426619 -0.00427336 -0.00427268 -0.00426743\n",
      " -0.00426943 -0.00426582 -0.00425869 -0.00426096 -0.00426746 -0.00426287\n",
      " -0.00425588 -0.00425852 -0.00425769 -0.00425632 -0.00426086 -0.00426824\n",
      " -0.00427102 -0.00427178 -0.00426863 -0.00426555 -0.00426282 -0.00426453\n",
      " -0.0042645  -0.00425432 -0.00425876 -0.00426213 -0.00426018 -0.00426262\n",
      " -0.00426733 -0.00427393 -0.00428354 -0.00428718 -0.00428062 -0.00428035\n",
      " -0.00427979 -0.00428074]\n",
      "[ 0.00068352  0.00067813  0.0006687   0.00067185  0.00068386  0.0006946\n",
      "  0.00069724  0.00069573  0.00069551  0.00069182  0.00069307  0.00069141\n",
      "  0.00068762  0.00069229  0.00069041  0.00068225  0.00068147  0.00068716\n",
      "  0.00068652  0.00069224  0.00069526  0.00069329  0.00068665  0.00069236\n",
      "  0.00069973  0.00069753  0.00069968  0.00070168  0.00069524  0.00068684\n",
      "  0.00068528  0.00068398  0.0006825   0.00068591  0.00068845  0.00068474\n",
      "  0.00068374  0.00069446  0.00069236  0.0006905   0.00069529  0.00069187\n",
      "  0.0006876   0.00067954  0.00066963  0.0006675   0.00067329  0.00067371\n",
      "  0.00067354  0.00067473]\n",
      "[-0.00148899 -0.00149429 -0.00150112 -0.00149724 -0.0014845  -0.00147546\n",
      " -0.00147209 -0.00147283 -0.00147393 -0.00148147 -0.00147676 -0.00147773\n",
      " -0.00148386 -0.00148149 -0.00148098 -0.0014884  -0.00148958 -0.00148135\n",
      " -0.00148398 -0.00148037 -0.00147695 -0.001478   -0.00148513 -0.00147957\n",
      " -0.00147249 -0.00147283 -0.00147092 -0.00146941 -0.00147351 -0.0014824\n",
      " -0.00148835 -0.0014916  -0.00148977 -0.00148748 -0.00148484 -0.00148855\n",
      " -0.00148955 -0.0014781  -0.00147859 -0.00147737 -0.0014741  -0.00147673\n",
      " -0.00148237 -0.0014894  -0.00149849 -0.00150168 -0.0014938  -0.00149241\n",
      " -0.00149229 -0.00149319]\n",
      "[ 0.00718806  0.00718066  0.00717397  0.00717634  0.00718933  0.00720154\n",
      "  0.00720168  0.00719915  0.00720156  0.00719675  0.00720171  0.00719722\n",
      "  0.00719182  0.0071936   0.00719382  0.00718679  0.00718665  0.00719146\n",
      "  0.00718906  0.00719448  0.00719824  0.00719553  0.0071906   0.00719587\n",
      "  0.00720203  0.00720115  0.00720173  0.00720481  0.00720051  0.00719275\n",
      "  0.00719001  0.00718794  0.00718669  0.00719177  0.00719451  0.00719045\n",
      "  0.0071894   0.00719832  0.0071967   0.00719441  0.00719607  0.00719573\n",
      "  0.00718999  0.00718369  0.00717329  0.0071717   0.00717695  0.00717922\n",
      "  0.00718066  0.00717957]\n",
      "[ 0.00717209  0.00716646  0.00716055  0.00716165  0.00717234  0.00718318\n",
      "  0.00718518  0.00718552  0.0071853   0.00718083  0.00718521  0.00718213\n",
      "  0.00717449  0.00717871  0.00717961  0.00717161  0.00717009  0.0071752\n",
      "  0.00717319  0.00717881  0.00718467  0.0071842   0.00717786  0.00718323\n",
      "  0.00719055  0.00718796  0.00719006  0.00719207  0.0071863   0.00717937\n",
      "  0.00717673  0.00717397  0.00717336  0.00717791  0.0071801   0.00717751\n",
      "  0.00717681  0.00718655  0.00718337  0.00718191  0.00718293  0.0071801\n",
      "  0.00717622  0.00716772  0.00715742  0.00715505  0.00716035  0.00716233\n",
      "  0.00716304  0.00716282]\n",
      "[ 0.00632468  0.00631685  0.00630815  0.00631335  0.00632473  0.00633386\n",
      "  0.00633713  0.00633552  0.00633418  0.00633015  0.0063356   0.00633442\n",
      "  0.00633005  0.00633286  0.00633201  0.00632439  0.00632268  0.00632764\n",
      "  0.0063269   0.00633271  0.00633633  0.00633372  0.0063282   0.00633601\n",
      "  0.00634358  0.00634006  0.00634236  0.00634729  0.00634041  0.00633167\n",
      "  0.00632937  0.0063271   0.00632678  0.00632805  0.0063302   0.00632727\n",
      "  0.00632598  0.00633967  0.00633489  0.00633049  0.00633342  0.0063324\n",
      "  0.00632979  0.00632129  0.00630781  0.00630515  0.00631318  0.00631687\n",
      "  0.00631528  0.00631355]\n",
      "[ 0.00399333  0.00398379  0.00397578  0.00398083  0.00399285  0.00400471\n",
      "  0.00400618  0.004005    0.00400474  0.00400183  0.00400786  0.00400342\n",
      "  0.0039989   0.00400273  0.00400105  0.0039907   0.00398801  0.00399595\n",
      "  0.00399526  0.00399814  0.00400381  0.00400154  0.00399578  0.00400344\n",
      "  0.00401086  0.0040093   0.00401248  0.00401628  0.00400959  0.00400129\n",
      "  0.0039989   0.00399717  0.00399568  0.00399646  0.00399968  0.00399656\n",
      "  0.00399644  0.00400667  0.00399993  0.00399792  0.00400212  0.00399851\n",
      "  0.00399668  0.00399106  0.00397798  0.00397483  0.00398296  0.00399075\n",
      "  0.00398999  0.00398748]\n",
      "[ 0.00044529  0.00043687  0.00042588  0.00043049  0.00044172  0.00045154\n",
      "  0.0004553   0.00045491  0.0004563   0.00045349  0.00045815  0.00045562\n",
      "  0.00045208  0.00045518  0.00045437  0.00044402  0.00044255  0.00044858\n",
      "  0.00044399  0.00044878  0.00045508  0.00045271  0.00044485  0.00045227\n",
      "  0.00046389  0.00046326  0.00046692  0.0004717   0.00046536  0.00045623\n",
      "  0.00045286  0.0004481   0.000447    0.00045002  0.00045227  0.0004489\n",
      "  0.00044673  0.00045901  0.00045461  0.00044919  0.00045188  0.00045173\n",
      "  0.00045005  0.00044106  0.00042886  0.00042678  0.00043701  0.0004408\n",
      "  0.00044214  0.00044031]\n",
      "[ 0.00420652  0.00420005  0.0041918   0.00419514  0.00420632  0.00421648\n",
      "  0.00421868  0.00421926  0.00421963  0.00421492  0.00421987  0.00421782\n",
      "  0.00421125  0.00420996  0.00420969  0.0042031   0.00420188  0.00420371\n",
      "  0.00420098  0.00420898  0.00421743  0.00421394  0.00420803  0.00421487\n",
      "  0.00422263  0.00422112  0.0042282   0.00423152  0.00422327  0.00421482\n",
      "  0.00421199  0.00420908  0.00420623  0.00421033  0.00421208  0.00421038\n",
      "  0.00420708  0.00421541  0.00421223  0.00421042  0.00421321  0.00421301\n",
      "  0.0042084   0.00419922  0.00418916  0.00418679  0.0041939   0.00419695\n",
      "  0.00419731  0.00419548]\n",
      "[ 0.01027048  0.01026348  0.01025581  0.01026135  0.01027356  0.01028386\n",
      "  0.01028694  0.01028923  0.01028716  0.01028152  0.01028525  0.01028198\n",
      "  0.01027668  0.01027876  0.01027883  0.01027039  0.0102678   0.01026924\n",
      "  0.0102644   0.01027231  0.01028003  0.01027834  0.01027371  0.01028235\n",
      "  0.01029133  0.01029221  0.010297    0.01030369  0.01029578  0.01028523\n",
      "  0.01027908  0.01027476  0.01027312  0.01027366  0.01027639  0.01027234\n",
      "  0.01027212  0.01027937  0.01027585  0.01027268  0.01027517  0.01027566\n",
      "  0.01027271  0.01026621  0.01025586  0.01025374  0.01026382  0.01026887\n",
      "  0.01026914  0.01026814]\n",
      "[ 0.0050342   0.0050248   0.0050167   0.00502253  0.00503376  0.00504348\n",
      "  0.00504646  0.00504763  0.00504771  0.0050426   0.00504609  0.00504482\n",
      "  0.00504221  0.00504722  0.00504583  0.00503557  0.00503264  0.00503804\n",
      "  0.00503696  0.00504194  0.00504556  0.00504387  0.00503674  0.00504402\n",
      "  0.00505466  0.00505505  0.00506121  0.00506516  0.00505955  0.00505071\n",
      "  0.00504514  0.00503989  0.00503645  0.00503684  0.00503958  0.00503704\n",
      "  0.00503916  0.00504939  0.005045    0.00504299  0.0050468   0.00504402\n",
      "  0.00504214  0.00503296  0.00502056  0.00502019  0.00503066  0.00503459\n",
      "  0.00503398  0.00503308]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "for e in el:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = scaler.transform(test)\n",
    "pred = autoencoder.predict(x)\n",
    "pred = scaler.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005175371620944706"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred - test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109828, 3050)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = autoencoder.predict(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build autoencoder model with Conv2D\n",
    "encoder = Sequential()\n",
    "encoder.add(Conv2D(nb_hidden, kernel_size=(12, 12), activation = 'relu', input_shape=(batch_size,1, nb_visible,1), init='he_normal'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Conv2D(nb_hidden, kernel_size=(7, 7), activation = 'relu', init='he_normal'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Conv2D(nb_hidden, kernel_size=(5, 5), activation = 'relu', init='he_normal'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(Conv2D(nb_hidden, kernel_size=(3, 3), activation = 'relu', init='he_normal'))\n",
    "encoder.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#encoder.add(Flatten())\n",
    "#encoder.add(Dense(180, activation = 'relu', init='he_normal'))\n",
    "#encoder.add(Dropout(0.5))\n",
    "#encoder.add(Dense(100, activation = 'relu', init='he_normal'))\n",
    "#encoder.add(Dropout(0.5))\n",
    "#encoder.add(Dense(nb_emb, activation = 'relu', init='he_normal'))\n",
    "\n",
    "decoder = Sequential()\n",
    "#decoder.add(Dense(nb_emb, activation = 'relu', init='he_normal'))\n",
    "#decoder.add(Dropout(0.5))\n",
    "#decoder.add(Dense(100, activation = 'relu', init='he_normal'))\n",
    "#decoder.add(Dropout(0.5))\n",
    "#decoder.add(Dense(180, activation = 'relu', init='he_normal'))\n",
    "d#ecoder.add(Flatten())\n",
    "decoder.add(UpSampling2D(pool_size=(2, 2)))\n",
    "decoder.add(Conv2D(nb_hidden, kernel_size=(3, 3), activation = 'relu', init='he_normal'))\n",
    "decoder.add(UpSampling2D(pool_size=(2, 2)))\n",
    "decoder.add(Conv2D(nb_hidden, kernel_size=(5, 5), activation = 'relu', init='he_normal'))\n",
    "decoder.add(UpSampling2D(pool_size=(2, 2)))\n",
    "decoder.add(Conv2D(nb_hidden, kernel_size=(7, 7), activation = 'relu', init='he_normal'))\n",
    "decoder.add(UpSampling2D(pool_size=(2, 2)))\n",
    "decoder.add(Conv2D(nb_hidden, kernel_size=(12, 12), activation = 'relu', init='he_normal'))\n",
    "\n",
    "\n",
    "input_img = Input(shape=(nb_visible,))\n",
    "encoded = encoder(input_img)\n",
    "decoded = decoder(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multilayer Perceptron model\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=625, input_dim=784, init=init_weights, activation='sigmoid', name='dense1'))\n",
    "model.add(Dropout(prob_drop_input, name='dropout1'))\n",
    "model.add(Dense(output_dim=625, input_dim=625, init=init_weights, activation='sigmoid', name='dense2'))\n",
    "model.add(Dropout(prob_drop_hidden, name='dropout2'))\n",
    "model.add(Dense(output_dim=10, input_dim=625, init=init_weights, activation='softmax', name='dense3'))\n",
    "model.compile(optimizer=RMSprop(lr=0.001, rho=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
